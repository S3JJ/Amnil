{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow pyngrok --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6Lm_4fvB4P2",
        "outputId": "d030fdb1-6443-42be-8cd0-0c95a5929997"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m752.6/752.6 kB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the Tiny ImageNet Dataset"
      ],
      "metadata": {
        "id": "KQOWRQlFoQ6v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsFgn4XFT5oZ",
        "outputId": "7c9caeef-4e15-4e6a-8e9f-ef04d655ba58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-28 06:39:40--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.64.64|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://cs231n.stanford.edu/tiny-imagenet-200.zip [following]\n",
            "--2025-10-28 06:39:41--  https://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248100043 (237M) [application/zip]\n",
            "Saving to: ‘tiny-imagenet-200.zip’\n",
            "\n",
            "tiny-imagenet-200.z 100%[===================>] 236.61M  3.82MB/s    in 57s     \n",
            "\n",
            "2025-10-28 06:40:38 (4.18 MB/s) - ‘tiny-imagenet-200.zip’ saved [248100043/248100043]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "!unzip -q tiny-imagenet-200.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the necessary packages"
      ],
      "metadata": {
        "id": "mj7DZgbIofCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "aPNTt_dpB82L"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up MLflow with ngrok"
      ],
      "metadata": {
        "id": "bpByT96y7XU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "ngrok_token = userdata.get(\"ngrokToken\")"
      ],
      "metadata": {
        "id": "p0dC5f6-YfD4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"MLFLOW_ENABLE_HOST_CHECKING\"] = \"false\""
      ],
      "metadata": {
        "id": "8RKPboxZavva"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pkill -f ngrok"
      ],
      "metadata": {
        "id": "fB40CHkJaObz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw(\"mlflow ui --port 2000 &\")\n",
        "mlflow.set_tracking_uri(\"http://localhost:2000\")\n",
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(ngrok_token)"
      ],
      "metadata": {
        "id": "YvR0eknsXqNO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "public_url = ngrok.connect(2000).public_url\n",
        "print(public_url)"
      ],
      "metadata": {
        "id": "CHvS0tELYtNe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "558e1a11-004e-46c9-835a-6b339b0ca229"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://dino-advisory-unprimitively.ngrok-free.dev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuring PyTorch to use the GPU if available"
      ],
      "metadata": {
        "id": "mQAtUhVAolF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vd0oIrGsUUEc",
        "outputId": "ab2176bd-afca-47e9-b9d4-f64e6c2b7a1f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the datset"
      ],
      "metadata": {
        "id": "dCgdMLn-o0UD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/tiny-imagenet-200\"\n",
        "num_classes = 200\n",
        "batch_size = 64\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=transform_train)\n",
        "val_dataset = datasets.ImageFolder(os.path.join(data_dir, \"val\"), transform=transform_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "Q1q7lsihENXm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the model"
      ],
      "metadata": {
        "id": "FqN-4TlWo2zW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.efficientnet_b0(weights='IMAGENET1K_V1')\n",
        "\n",
        "model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "O0CjMFBjU3EQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model"
      ],
      "metadata": {
        "id": "dtagQRIzpABz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.set_experiment(\"EfficientNet_TinyImageNet\")\n",
        "\n",
        "with mlflow.start_run(run_name=\"EfficientNet-B0-TinyImageNet\"):\n",
        "\n",
        "    mlflow.log_param(\"model\", \"efficientnet_b0\")\n",
        "    mlflow.log_param(\"optimizer\", \"Adam\")\n",
        "    mlflow.log_param(\"learning_rate\", 0.001)\n",
        "    mlflow.log_param(\"batch_size\", batch_size)\n",
        "    mlflow.log_param(\"num_classes\", num_classes)\n",
        "\n",
        "    num_epochs = 5\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        for images, labels in tqdm(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_acc = 100 * correct / total\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        # Log metrics\n",
        "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
        "        mlflow.log_metric(\"train_accuracy\", train_acc, step=epoch)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {avg_train_loss:.4f} | Acc: {train_acc:.2f}%\")\n",
        "\n",
        "    # Save model with MLflow\n",
        "    mlflow.pytorch.log_model(model, \"model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3tLvTxEC2UD",
        "outputId": "d78d76dd-f72c-443f-b3bc-6722d05ee3e2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/10/28 04:26:14 INFO mlflow.tracking.fluent: Experiment with name 'EfficientNet_TinyImageNet' does not exist. Creating a new experiment.\n",
            "100%|██████████| 1563/1563 [11:35<00:00,  2.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5] Loss: 2.8617 | Acc: 35.30%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [11:28<00:00,  2.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/5] Loss: 2.2814 | Acc: 46.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [11:32<00:00,  2.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/5] Loss: 2.0887 | Acc: 50.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [11:32<00:00,  2.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/5] Loss: 1.9654 | Acc: 52.67%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [11:30<00:00,  2.26it/s]\n",
            "2025/10/28 05:23:54 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/5] Loss: 1.8875 | Acc: 54.28%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/10/28 05:23:55 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/10/28 05:24:04 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.23.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torchvision==0.23.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "\u001b[31m2025/10/28 05:24:04 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏃 View run EfficientNet-B0-TinyImageNet at: http://localhost:2000/#/experiments/224451195676655221/runs/0efbeb13115647e79557294347de6a71\n",
            "🧪 View experiment at: http://localhost:2000/#/experiments/224451195676655221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "val_correct, val_total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        val_correct += (preds == labels).sum().item()\n",
        "        val_total += labels.size(0)\n",
        "val_acc = 100 * val_correct / val_total\n",
        "\n",
        "mlflow.log_metric(\"val_accuracy\", val_acc)"
      ],
      "metadata": {
        "id": "xC8W9OesDGCY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the model"
      ],
      "metadata": {
        "id": "M5P3OxeCpBZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"efficientnet_b0_tinyimagenet.pth\")\n",
        "print(\"Model saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHcTXl8-VEYB",
        "outputId": "aa081878-14f8-484d-8d99-4b741068a2e3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the model and testing on custom data"
      ],
      "metadata": {
        "id": "dgw3ZkUHpE8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping from wnid to class name\n",
        "label_dict = {}\n",
        "with open(\"/content/tiny-imagenet-200/words.txt\", \"r\") as f:\n",
        "    for line in f:\n",
        "        wnid, name = line.strip().split(\"\\t\")\n",
        "        label_dict[wnid] = name\n",
        "\n",
        "# Loading the trained model\n",
        "model = models.efficientnet_b0(weights=None)\n",
        "model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
        "model.load_state_dict(torch.load(\"efficientnet_b0_tinyimagenet.pth\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Preprocessing image\n",
        "transform_infer = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "img_path = \"/content/example2.jpg\"\n",
        "image = Image.open(img_path).convert(\"RGB\")\n",
        "image_t = transform_infer(image).unsqueeze(0).to(device)\n",
        "\n",
        "# Predicting on custom data\n",
        "with torch.no_grad():\n",
        "    outputs = model(image_t)\n",
        "    probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "    pred_class = probs.argmax(dim=1).item()\n",
        "\n",
        "# Getting class name from wnid\n",
        "idx_to_class = {v: k for k, v in train_dataset.class_to_idx.items()}\n",
        "wnid = idx_to_class[pred_class]\n",
        "class_name = label_dict.get(wnid, \"Unknown class\")\n",
        "\n",
        "print(f\"Predicted class: {class_name} ({wnid})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JUfM7NwZPC8",
        "outputId": "71d16667-2523-4344-b0a0-70c7a55b1ff4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: scorpion (n01770393)\n"
          ]
        }
      ]
    }
  ]
}